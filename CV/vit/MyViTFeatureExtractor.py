import torch
from transformers import ViTModel, ViTConfig
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
from typing import List, Union
import logging
import os
import torch.nn.functional as F

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class MyViTFeatureExtractor:
    def __init__(self, local_model_path: str) -> None:
        """
        åˆå§‹åŒ–ç‰¹å¾æå–å™¨ã€‚

        é€‚é…: èƒ½å¤ŸåŠ è½½ç”± MetricViT è®­ç»ƒå¹¶ä¿å­˜çš„ backbone æ¨¡å‹ã€‚
        """
        if not os.path.isdir(local_model_path):
            raise NotADirectoryError(f"Model path not found: {local_model_path}")

        logging.info(f"Loading model from: {local_model_path}")

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logging.info(f"Using device: {self.device}")

        try:
            # åŠ è½½é…ç½®å’Œæ¨¡å‹ (è¿™é‡ŒåŠ è½½çš„æ˜¯çº¯ ViTModel)
            self.config = ViTConfig.from_pretrained(local_model_path, local_files_only=True)
            self.model = ViTModel.from_pretrained(local_model_path, config=self.config, local_files_only=True)
        except Exception as e:
            logging.error(f"Failed to load model: {e}")
            raise

        self.model.to(self.device)
        self.model.eval()

        # å®šä¹‰é¢„å¤„ç† (ä¸è®­ç»ƒæ—¶çš„ Val Transform ä¿æŒä¸€è‡´)
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ])

        self.feature_dim = self.model.config.hidden_size
        logging.info(f"Feature dimension: {self.feature_dim}")

    def run(self, imgs: List[Union[str, np.ndarray, Image.Image]], normalize: bool = True) -> np.ndarray:
        """
        å¤„ç†ä¸€æ‰¹å›¾åƒå¹¶è¿”å›ç‰¹å¾å‘é‡ã€‚

        Args:
            imgs: å›¾ç‰‡è·¯å¾„ã€numpyæ•°ç»„æˆ–PIL Imageçš„åˆ—è¡¨
            normalize: æ˜¯å¦è¿›è¡Œ L2 å½’ä¸€åŒ– (å¼ºçƒˆå»ºè®®ä¸º Trueï¼Œé€‚é… Milvus/Cosine æœç´¢)
        """
        if not imgs:
            return np.empty((0, self.feature_dim), dtype=np.float32)

        processed_tensors = []
        valid_indices = []

        # 1. é¢„å¤„ç†
        for i, img_input in enumerate(imgs):
            try:
                # --- å›¾åƒè¯»å–é€»è¾‘ (ä¿æŒä½ åŸæœ‰çš„å¥å£®æ€§é€»è¾‘) ---
                if isinstance(img_input, str):
                    img = Image.open(img_input).convert('RGB')
                elif isinstance(img_input, np.ndarray):
                    if img_input.dtype != np.uint8:
                        if img_input.max() <= 1.0:
                            img_input = (img_input * 255).astype(np.uint8)
                        else:
                            img_input = img_input.astype(np.uint8)
                    img = Image.fromarray(img_input, 'RGB')
                elif isinstance(img_input, Image.Image):
                    img = img_input.convert('RGB')
                else:
                    continue
                    # ----------------------------------------

                img_tensor = self.transform(img)
                processed_tensors.append(img_tensor)
                valid_indices.append(i)
            except Exception as e:
                logging.error(f"Error processing image index {i}: {e}")

        if not processed_tensors:
            return np.empty((0, self.feature_dim), dtype=np.float32)

        # 2. æ¨ç†
        batch_tensor = torch.stack(processed_tensors, dim=0).to(self.device)

        with torch.no_grad():
            outputs = self.model(batch_tensor)
            # ã€å…³é”®ã€‘æå– last_hidden_state çš„ [CLS] token (Index 0)
            # è¿™ä¸è®­ç»ƒæ—¶çš„ MetricViT ä¿æŒå®Œå…¨ä¸€è‡´
            features = outputs.last_hidden_state[:, 0, :]

        # 3. åå¤„ç†
        if normalize:
            # ä½¿ç”¨ PyTorch çš„ normalize æ›´ç²¾ç¡®ï¼Œæˆ–è€…ä¿æŒ numpy å®ç°
            features = F.normalize(features, p=2, dim=1)
            output_np = features.cpu().numpy()
        else:
            output_np = features.cpu().numpy()

        # 4. å¡«å……ç»“æœ (ä¿æŒåˆ—è¡¨é•¿åº¦ä¸€è‡´)
        if len(output_np) != len(imgs):
            final_output = np.full((len(imgs), self.feature_dim), np.nan, dtype=np.float32)
            for idx, vec in zip(valid_indices, output_np):
                final_output[idx] = vec
            return final_output

        return output_np


def compare_images(extractor, img_path_A, img_path_B):
    """
    è®¡ç®—ä¸¤å¼ å›¾ç‰‡çš„ç›¸ä¼¼åº¦
    """
    if not os.path.exists(img_path_A) or not os.path.exists(img_path_B):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡è·¯å¾„ã€‚\nA: {img_path_A}\nB: {img_path_B}")
        return

    print(f"ğŸ” æ­£åœ¨å¯¹æ¯”:")
    print(f"  å›¾ A: {os.path.basename(img_path_A)}")
    print(f"  å›¾ B: {os.path.basename(img_path_B)}")

    # 1. æå–ç‰¹å¾ (ä¸€æ¬¡ä¼ å…¥ä¸¤å¼ å›¾ï¼Œæ•ˆç‡æ›´é«˜)
    # run æ–¹æ³•è¿”å›çš„æ˜¯å·²ç»å½’ä¸€åŒ–è¿‡çš„ numpy æ•°ç»„
    vectors = extractor.run([img_path_A, img_path_B], normalize=True)

    vec_a = vectors[0]
    vec_b = vectors[1]

    # 2. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
    # å› ä¸º vec_a å’Œ vec_b æ¨¡é•¿éƒ½ä¸º 1ï¼Œæ‰€ä»¥ç‚¹ç§¯å°±æ˜¯ä½™å¼¦ç›¸ä¼¼åº¦
    similarity = np.dot(vec_a, vec_b)

    # 3. è®¡ç®—æ¬§æ°è·ç¦» (Euclidean Distance) - è¾…åŠ©å‚è€ƒ
    # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼
    distance = np.linalg.norm(vec_a - vec_b)

    # 4. æ‰“å°ç»“æœ
    print("-" * 30)
    print(f"ğŸ“Š ç›¸ä¼¼åº¦ç»“æœ:")
    print(f"  â˜… ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine): {similarity:.4f}  (è¶Šæ¥è¿‘ 1.0 è¶Šç›¸ä¼¼)")
    print(f"  â˜† æ¬§æ°è·ç¦» (L2 Dist):  {distance:.4f}    (è¶Šæ¥è¿‘ 0.0 è¶Šç›¸ä¼¼)")
    print("-" * 30)

    # 5. ç®€å•åˆ¤å®šå»ºè®®
    threshold = 0.85  # è¿™ä¸ªé˜ˆå€¼å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    if similarity > threshold:
        print("âœ… ç»“è®º: å®ƒä»¬ææœ‰å¯èƒ½æ˜¯åŒä¸€å¼ å¡ (æˆ–åŒä¸€å®å¯æ¢¦çš„ä¸åŒè¯­è¨€ç‰ˆæœ¬)")
    else:
        print("âŒ ç»“è®º: å®ƒä»¬çœ‹èµ·æ¥æ˜¯ä¸åŒçš„å¡ç‰‡")
    print("\n")


if __name__ == "__main__":
    # ================= é…ç½® =================
    # ä½ çš„æ¨¡å‹ä¿å­˜è·¯å¾„
    MODEL_PATH = "/home/martin/ML/Model/pokemon_cls/vit-base-patch16-224-Pokemon03"

    # è¿™é‡Œå¡«å…¥ä½ æƒ³æµ‹è¯•çš„ä¸¤å¼ å›¾ç‰‡çš„ç»å¯¹è·¯å¾„
    # å»ºè®®æµ‹è¯•ï¼š
    # 1. ä¸€å¼ ä¸­æ–‡å¡ vs åŒä¸€å¼ çš„è‹±æ–‡å¡
    # 2. ä¸€å¼ å¡ vs ä¸€å¼ å®Œå…¨ä¸åŒçš„å¡
    IMG_1 = r"/home/martin/ML/RemoteProject/untitled10/uploads/ä¼Šå¸ƒus1.png"
    IMG_2 = r"/home/martin/ML/RemoteProject/untitled10/uploads/ä¼Šå¸ƒtc1.png"

    # ================= è¿è¡Œ =================
    try:
        print("æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        # åˆå§‹åŒ–æå–å™¨
        extractor = MyViTFeatureExtractor(MODEL_PATH)

        # æ‰§è¡Œå¯¹æ¯”
        compare_images(extractor, IMG_1, IMG_2)

    except Exception as e:
        print(f"è¿è¡Œå‡ºé”™: {e}")